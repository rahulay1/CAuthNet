{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44075b9",
   "metadata": {},
   "source": [
    "# Extracting Data from H-MOG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2e7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "    \n",
    "directory = \"C:/Users/rahul/PycharmProjects/siamese-triplet/hmog_dataset/public_dataset_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf3933",
   "metadata": {},
   "source": [
    "## function to extract X, Y, Z readings from each CSV files of H-MOG dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51389fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting acc, gy, and mag reading from CSV files. Columns 7, 8, 9 contains the X, Y, Z readings\n",
    "def get_activity(concatenated_rows, df_activity, df_TouchEvent, row_te, allin):\n",
    "  for index_act, row_act in df_activity[::-1].iterrows():\n",
    "    if row_te[0] > row_act[3]:\n",
    "      act_cols = [7, 8, 9]\n",
    "      act_row = df_activity.iloc[index_act]\n",
    "\n",
    "      concatenated_row = [row_te[col] for col in df_TouchEvent.columns] + [row_act[col] for col in act_cols]\n",
    "      concatenated_row.extend([allin[col] for col in allin.columns])\n",
    "\n",
    "      concatenated_rows.append(concatenated_row)\n",
    "\n",
    "      break\n",
    "\n",
    "  return concatenated_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa76fc",
   "metadata": {},
   "source": [
    "## Function to extract 144 features for each touch event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b093957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_mean, X_std, X_min, X_max, Y_mean, Y_std, Y_min, Y_max, Z_mean, Z_std, Z_min, Z_max, M_mean, M_std, M_min, M_max\n",
    "# 0 - 47 - acc; 48-95 - gyro; 96 - 143 - mag\n",
    "# 0-15 before\n",
    "# 16-31 - after\n",
    "# 32 - 47 - difference\n",
    "\n",
    "def get_sensor_data(row_te,df_acc):\n",
    "    \n",
    "    stat_before = None\n",
    "    stat_after = None\n",
    "    diff = None\n",
    "    \n",
    "    t_min = row_te[0]-100\n",
    "    t = row_te[0]\n",
    "    t_max = row_te[0]+100\n",
    "    \n",
    "    cols_keep = [3,4,5]\n",
    "    stats_col_keep = [\"mean\", \"std\",  \"min\",   \"max\"]\n",
    "    \n",
    "    # Filter rows where elements in the first column are between min and max\n",
    "    filtered_df_acc_before = df_acc[ (df_acc[0] >= t_min) & (df_acc[0] <= t) ]\n",
    "    filtered_df_acc_after = df_acc[ (df_acc[0] >= t) & (df_acc[0] <= t_max) ]\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Check for empty DataFrames before selecting columns\n",
    "    if not filtered_df_acc_before.empty and not filtered_df_acc_after.empty:\n",
    "        filtered_df_acc_before = filtered_df_acc_before[cols_keep]\n",
    "        filtered_df_acc_before['mean_sqrt'] = (filtered_df_acc_before.iloc[:, :3] ** 2).mean(axis=1) ** 0.5\n",
    "#         print(filtered_df_acc_before)\n",
    "        # Calculate statistics for each column (excluding percentiles)\n",
    "        stats_before = filtered_df_acc_before.describe(percentiles=[]).transpose()  \n",
    "        stats_before = stats_before[stats_col_keep]\n",
    "#         print(\"xxx\",stats_before)\n",
    "        stats_before=stats_before.values.flatten()\n",
    "#         acc_stat_before = stats_before.values.flatten().tolist()\n",
    "    \n",
    "#     if not filtered_df_acc_after.empty:\n",
    "        filtered_df_acc_after = filtered_df_acc_after[cols_keep]\n",
    "        filtered_df_acc_after['mean_sqrt'] = (filtered_df_acc_after.iloc[:, :3] ** 2).mean(axis=1) ** 0.5\n",
    "        # Calculate statistics for each column (excluding percentiles)\n",
    "        stats_after = filtered_df_acc_after.describe(percentiles=[]).transpose()  \n",
    "        stats_after = stats_after[stats_col_keep]\n",
    "        stats_after =  stats_after.values.flatten()\n",
    "#         # Flatten the DataFrame into a list\n",
    "#         acc_stat_after = stats_after.values.flatten().tolist()\n",
    "        diff= stats_before-stats_after\n",
    "        return stats_before.tolist() + stats_after.tolist() + diff.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230fec3a",
   "metadata": {},
   "source": [
    "## This function goes through every folder and CSV files in H-MOG dataset and extract features for all touch events and store it as .pkl files\n",
    "\n",
    "## we are limiting the number of touch event per CSV file to 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "for root, _, files in os.walk(directory):\n",
    "    concatenated_rows = []\n",
    "#     contains_X_all = any(\"X_all1.pkl\" in file for file in files)\n",
    "#     contains_X_300 = any(\"X_3001.pkl\" in file for file in files)\n",
    "\n",
    "    contains_activity_csv = any(\"Activity.csv\" in file for file in files)\n",
    "    if contains_activity_csv:# and not contains_X_300:\n",
    "        filepath_act = os.path.join(root, \"Activity.csv\")\n",
    "        df_activity = pd.read_csv(filepath_act, header=None)  \n",
    "        if os.path.getsize(os.path.join(root, \"X_300.pkl\")) < 2048:\n",
    "\n",
    "            # extracting sessions related to 2, 8, 14, 20 (Walking and readings)\n",
    "            if df_activity.iloc[0, 8] in [2,8,14,20]:\n",
    "                print(filepath_act)\n",
    "                filepath_TouchEvent = os.path.join(root, \"TouchEvent.csv\")\n",
    "                filepath_acc = os.path.join(root, \"Accelerometer.csv\")\n",
    "                filepath_gyro = os.path.join(root, \"Gyroscope.csv\")\n",
    "                filepath_mag = os.path.join(root, \"Magnetometer.csv\")\n",
    "\n",
    "                df_TouchEvent = pd.read_csv(filepath_TouchEvent, header=None)\n",
    "                df_acc = pd.read_csv(filepath_acc, header=None)\n",
    "                df_gyro = pd.read_csv(filepath_gyro, header=None)\n",
    "                df_mag = pd.read_csv(filepath_mag, header=None)\n",
    "\n",
    "                for index_te, row_te in df_TouchEvent.iterrows():\n",
    "                    acc = get_sensor_data(row_te, df_acc)\n",
    "                    gyro = get_sensor_data(row_te, df_gyro)\n",
    "                    mag = get_sensor_data(row_te, df_mag)\n",
    "\n",
    "                    if acc is not None and gyro is not None and mag is not None:\n",
    "                        allin = pd.DataFrame(acc + gyro + mag).transpose()\n",
    "                        has_nan = allin.isnull().values.any()\n",
    "\n",
    "                        if not has_nan:\n",
    "                            concatenated_rows = get_activity(concatenated_rows, df_activity, df_TouchEvent, row_te, allin)\n",
    "                            test = pd.DataFrame(concatenated_rows)\n",
    "                            if len(concatenated_rows) > 300:\n",
    "                                break\n",
    "\n",
    "                df_C = pd.DataFrame(concatenated_rows)\n",
    "                save_path = os.path.join(root, \"X_300_W_R.pkl\")\n",
    "                df_C.to_pickle(save_path)\n",
    "                print(df_C.shape)\n",
    "                print(root)\n",
    "\n",
    "                # Clearing memory\n",
    "                del df_activity\n",
    "                del df_TouchEvent\n",
    "                del df_acc\n",
    "                del df_gyro\n",
    "                del df_mag\n",
    "                del df_C\n",
    "                del test\n",
    "                del concatenated_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e4b2f",
   "metadata": {},
   "source": [
    "## This code go through all the folders and to read .pkl and create a large data frame X and corresponding label. \n",
    "\n",
    "## For example, the first row of X contain 144 features for user 0's first touch event.\n",
    "\n",
    "## the second row of X contain 144 features for user 0's second touch event.\n",
    "\n",
    "## 301st row of X contain 144 features for user 1's first touch event.\n",
    "\n",
    "## the outputs are stored as .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def min_max_normalize(df):\n",
    "    min_values = df.min(axis=0)  # Find minimum along each column\n",
    "    max_values = df.max(axis=0)  # Find maximum along each column\n",
    "    normalized = (df - min_values) / (max_values - min_values)\n",
    "    return normalized\n",
    "\n",
    "def standard_normalize(df):\n",
    "    mean = df.mean(axis=0)  # Calculate mean along each column\n",
    "    std = df.std(axis=0)  # Calculate standard deviation along each column\n",
    "    normalized = (df - mean) / std\n",
    "    return normalized, mean, std\n",
    "\n",
    "def get_folder_names(directory):\n",
    "    folders = []\n",
    "    for root, dirnames, _ in os.walk(directory):\n",
    "        # Access dirnames directly, skipping subdirectory iteration\n",
    "        folders.extend(dirnames)  # Use extend to efficiently append all names\n",
    "        break  # Exit the loop after processing the top-level directory\n",
    "    return folders\n",
    "\n",
    "def split_folders(folder_names, split_ratio):\n",
    "    split_index = int(len(folder_names) * split_ratio)\n",
    "    first_split = folder_names[:split_index]\n",
    "    second_split = folder_names[split_index:]\n",
    "    return first_split, second_split\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# Update this folder #########################################################################\n",
    "directory = \"C:/Users/rahul/PycharmProjects/siamese-triplet/hmog_dataset/public_dataset_test/\"\n",
    "###############################################################################################\n",
    "folder_names = get_folder_names(directory)\n",
    "\n",
    "# Split folders into two arrays (can adjust split_ratio as needed)\n",
    "training, testing = split_folders(folder_names, split_ratio=1)\n",
    "\n",
    "\n",
    "concatenated_rows = []\n",
    "for file in training:\n",
    "    filepath = os.path.join(directory, file)\n",
    "    for root, _, files in os.walk(filepath):\n",
    "            for file in files:\n",
    "                if file == \"X_300.pkl\":  # Check directly for the filename\n",
    "                    filepath_act = os.path.join(root, file)\n",
    "                    X = pd.read_pickle(filepath_act)\n",
    "                    concatenated_rows.append(X)\n",
    "                    print(filepath)\n",
    "\n",
    "# Check if any data was loaded before creating the DataFrame\n",
    "if concatenated_rows:\n",
    "    df_C = pd.concat(concatenated_rows, ignore_index=True)  # Concatenate DataFrames\n",
    "    print(df_C.shape)\n",
    "else:\n",
    "    print(\"No 'X_300.pkl' files found in the directory.\")\n",
    "    \n",
    "df_C = df_C.astype(dtype='float64')\n",
    "df_C.shape\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "y = df_C.iloc[:,2]\n",
    "y = (y/10**9).astype(int)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y= encoder.fit_transform(y)\n",
    "y=pd.DataFrame(y)\n",
    "X = df_C.iloc[:,14:159]\n",
    "\n",
    "\n",
    "\n",
    "X,mean,std = standard_normalize(X)\n",
    "\n",
    "X_ori = X.to_numpy()\n",
    "np.save('X_100_all.npy', X_ori)\n",
    "y_ori = y.to_numpy()\n",
    "np.save('y_100_all.npy', y_ori)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
